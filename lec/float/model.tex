\section{Modeling floating point}

The fact that normal floating point results have a relative error
bounded by $\macheps$ gives us a useful {\em model} for reasoning about
floating point error.  We will refer to this as the ``$1 + \delta$''
model.  For example, suppose $x$ is an exactly-represented input to
the MATLAB statement
\begin{lstlisting}
    z = 1-x*x;
\end{lstlisting}
We can reason about the error in the computed $\hat{z}$ as follows:
\begin{align*}
  t_1 &= \fl(x^2) = x^2 (1+\delta_1) \\
  t_2 &= 1-t_1 = (1-x^2)\left( 1 + \frac{\delta_1 x^2}{1-x^2} \right) \\
  \hat{z}
  &= \fl(1-t_1)
    = z \left( 1 + \frac{\delta_1 x^2}{1-x^2} \right)(1+\delta_2) \\
  & \approx z \left( 1 + \frac{\delta_1 x^2}{1-x^2} +\delta_2 \right),
\end{align*}
where $|\delta_1|, |\delta_2| \leq \macheps$.  As before, we throw
away the (tiny) term involving $\delta_1 \delta_2$.
Note that if $z$ is close to zero (i.e.~if there is {\em cancellation} in the
subtraction), then the model shows the result may have a
large relative error.

\subsection{First-order error analysis}

Analysis in the $1+\delta$ model quickly gets to be a sprawling mess
of Greek letters unless one is careful.  A standard trick to get
around this is to use {\em first-order} error analysis in which we
linearize all expressions involving roundoff errors.  In particular,
we frequently use the approximations
\begin{align*}
  (1+\delta_1)(1+\delta_2) & \approx 1+\delta_1 + \delta_2 \\
  1/(1+\delta) & \approx 1-\delta.
\end{align*}
In general, we will resort to first-order analysis without comment.
Those students who think this is a sneaky trick to get around our
lack of facility with algebra\footnote{%
Which it is.
}
may take comfort in the fact that if $|\delta_i| < \macheps$, then
in double precision
\[
  \left| \prod_{i=1}^n (1+\delta_i) \prod_{i=n+1}^N (1+\delta_i)^{-1} \right| < (1+1.03 N \macheps)
\]
for $N < 10^{14}$ (and a little further).

\subsection{Shortcomings of the model}

The $1+\delta$ model has two shortcomings.  First, it is only valid
for expressions that involve normalized numbers --- most notably,
gradual underflow breaks the model.  Second, the model is sometimes
pessimistic.  Certain operations, such as taking a difference between
two numbers within a factor of $2$ of each other, multiplying or
dividing by a factor of two\footnote{Assuming that the result
does not overflow or produce a subnormal.}, or multiplying two
single-precision numbers into a double-precision result,
are {\em exact} in floating point.  There are useful operations
such as simulating extended precision using ordinary floating point
that rely on these more detailed properties of the floating point system,
and cannot be analyzed using just the $1+\delta$ model.
